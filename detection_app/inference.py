import torch
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
import yaml
import os
from M2TR.models.m2tr import M2TR
import numpy as np
from typing import Union, Tuple, List
import cv2  # æ·»åŠ  cv2 å¯¼å…¥
import time

class DeepfakeDetector:
    def __init__(self, model_path):
        # åŠ è½½é…ç½®æ–‡ä»¶
        with open('../configs/m2tr.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        #print(f"\nä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡åž‹
        print("\næ­£åœ¨åˆå§‹åŒ–æ¨¡åž‹...")
        self.model = M2TR(config['MODEL'])
        
        # æ‰“å°æ¨¡åž‹ç»“æž„
        # print("\næ¨¡åž‹æž¶æž„:")
        # print(self.model)
        
        # åŠ è½½æ¨¡åž‹æƒé‡
        print(f"\næ­£åœ¨åŠ è½½æƒé‡æ–‡ä»¶: {model_path}")
        checkpoint = torch.load(model_path, map_location=self.device)

        # èŽ·å–æ¨¡åž‹çŠ¶æ€å­—å…¸
        if 'model_state' in checkpoint:
            state_dict = checkpoint['model_state']
            print("æ‰¾åˆ°æ¨¡åž‹çŠ¶æ€å­—å…¸")
        else:
            raise ValueError("æƒé‡æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'model_state' é”®")

        # èŽ·å–å½“å‰æ¨¡åž‹çš„çŠ¶æ€å­—å…¸
        model_state = self.model.state_dict()
        print("\næ¨¡åž‹æž¶æž„ä¸Žæƒé‡åŒ¹é…æ£€æŸ¥:")
    
        # æ£€æŸ¥å½¢çŠ¶åŒ¹é…
        shape_mismatch = []
        missing_keys = []
        unexpected_keys = []
        
        # æ£€æŸ¥æƒé‡æ–‡ä»¶ä¸­çš„é”®æ˜¯å¦éƒ½åœ¨æ¨¡åž‹ä¸­å­˜åœ¨ï¼Œå¹¶ä¸”å½¢çŠ¶åŒ¹é…
        for key, weight in state_dict.items():
            if key not in model_state:
                unexpected_keys.append(key)
                continue
            if weight.shape != model_state[key].shape:
                shape_mismatch.append(f"{key}: æƒé‡å½¢çŠ¶ {weight.shape} vs æ¨¡åž‹å½¢çŠ¶ {model_state[key].shape}")
        
        # æ£€æŸ¥æ¨¡åž‹ä¸­çš„é”®æ˜¯å¦éƒ½åœ¨æƒé‡æ–‡ä»¶ä¸­å­˜åœ¨
        for key in model_state.keys():
            if key not in state_dict:
                missing_keys.append(key)
        
        # æ‰“å°æ£€æŸ¥ç»“æžœ
        if len(missing_keys) > 0:
            print("\nç¼ºå¤±çš„æƒé‡:")
            for key in missing_keys:
                print(f"- {key}")
        
        if len(unexpected_keys) > 0:
            print("\nå¤šä½™çš„æƒé‡:")
            for key in unexpected_keys:
                print(f"- {key}")
        
        if len(shape_mismatch) > 0:
            print("\nå½¢çŠ¶ä¸åŒ¹é…çš„å±‚:")
            for mismatch in shape_mismatch:
                print(f"- {mismatch}")
        
        if len(missing_keys) == 0 and len(unexpected_keys) == 0 and len(shape_mismatch) == 0:
            print("æ¨¡åž‹æž¶æž„ä¸Žæƒé‡å®Œå…¨åŒ¹é…ï¼")
        
        
        # åŠ è½½å¤„ç†åŽçš„æƒé‡
        try:
            # missing_keys, unexpected_keys = self.model.load_state_dict(new_state_dict, strict=False)
            self.model.load_state_dict(state_dict)
            print("\næƒé‡åŠ è½½å®Œæˆ")
            # print(f"ç¼ºå¤±çš„é”®: {missing_keys}")
            # print(f"æœªé¢„æœŸçš„é”®: {unexpected_keys}")
        except Exception as e:
            print("\næƒé‡åŠ è½½å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:")
            print(e)
        
        # å°†æ¨¡åž‹ç§»è‡³æŒ‡å®šè®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.to(self.device)
        self.model.eval()
        
        # è®¾ç½®å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((380, 380)),  # ä¸Žé…ç½®æ–‡ä»¶ä¸­çš„IMG_SIZEä¸€è‡´
            transforms.CenterCrop(320),     # ä¸ŽMODEL.IMG_SIZEä¸€è‡´
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                              std=[0.229, 0.224, 0.225])
        ])

        
        print("\nåˆå§‹åŒ–å®Œæˆ")



    def add_scan_effect(self, image):
        """æ·»åŠ æ‰«æåŠ¨ç”»æ•ˆæžœ"""
        img = image.copy()
        draw = ImageDraw.Draw(img)
        
        # æ·»åŠ è¾¹æ¡†
        width, height = img.size
        border = 3
        draw.rectangle([(0, 0), (width-1, height-1)], outline='cyan', width=border)
        
        # æ·»åŠ è§’æ ‡
        corner_length = 20
        # å·¦ä¸Šè§’
        draw.line([(0, 0), (corner_length, 0)], fill='cyan', width=border)
        draw.line([(0, 0), (0, corner_length)], fill='cyan', width=border)
        # å³ä¸Šè§’
        draw.line([(width-1, 0), (width-1-corner_length, 0)], fill='cyan', width=border)
        draw.line([(width-1, 0), (width-1, corner_length)], fill='cyan', width=border)
        # å·¦ä¸‹è§’
        draw.line([(0, height-1), (corner_length, height-1)], fill='cyan', width=border)
        draw.line([(0, height-1), (0, height-1-corner_length)], fill='cyan', width=border)
        # å³ä¸‹è§’
        draw.line([(width-1, height-1), (width-1-corner_length, height-1)], fill='cyan', width=border)
        draw.line([(width-1, height-1), (width-1, height-1-corner_length)], fill='cyan', width=border)
        
        return img


    def predict(self, input_data):
        """ç»Ÿä¸€çš„é¢„æµ‹æŽ¥å£ï¼Œæ”¯æŒå›¾ç‰‡å’Œè§†é¢‘è¾“å…¥"""
        # å¤„ç†è§†é¢‘è¾“å…¥
        if isinstance(input_data, str) and input_data.lower().endswith(('.mp4', '.avi', '.mov')):
            print("\nðŸŽ¥ å¼€å§‹å¤„ç†è§†é¢‘...")
            frames = self.extract_frames(input_data)
            frame_predictions = []
            fake_probs = []
            processed_frames = []  # å­˜å‚¨å¤„ç†åŽçš„å¸§
            
            for i, frame in enumerate(frames):
                print(f"\nâ³ æ­£åœ¨åˆ†æžç¬¬ {i+1}/{len(frames)} å¸§...")
                # æ·»åŠ æ‰«ææ•ˆæžœ
                frame_with_effect = self.add_scan_effect(frame)
                processed_frames.append(frame_with_effect)
                
                # å¯¹æ¯ä¸€å¸§è¿›è¡Œé¢„æµ‹
                image = self.transform(frame).unsqueeze(0)
                image = image.to(self.device)
                
                with torch.no_grad():
                    print("ðŸ” è¿è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹...")
                    output = self.model({'img': image})
                    logits = output['logits']
                    probs = torch.softmax(logits, dim=1)
                    fake_prob = float(probs[0][1].cpu())
                    print(f"ðŸ“Š å½“å‰å¸§ä¼ªé€ æ¦‚çŽ‡: {fake_prob:.4f}")
                    fake_probs.append(fake_prob)

                time.sleep(0.1)  # æ·»åŠ å°å»¶è¿Ÿä»¥å±•ç¤ºè¿›åº¦
            
            # è®¡ç®—å¹³å‡ä¼ªé€ æ¦‚çŽ‡
            avg_fake_prob = np.mean(fake_probs)
            is_fake = avg_fake_prob > 0.5
            print(f"\nâœ¨ è§†é¢‘åˆ†æžå®Œæˆï¼å¹³å‡ä¼ªé€ æ¦‚çŽ‡: {avg_fake_prob:.4f}")
            return is_fake, avg_fake_prob

        # å¤„ç†å›¾ç‰‡è¾“å…¥
        print("\nðŸ“¸ å¼€å§‹å¤„ç†å›¾ç‰‡...")
        if isinstance(input_data, str):
            input_data = Image.open(input_data).convert('RGB')
        # æ·»åŠ æ‰«ææ•ˆæžœ
        input_data_with_effect = self.add_scan_effect(input_data)
        print("ðŸ”„ é¢„å¤„ç†å›¾åƒ...")
        image = self.transform(input_data).unsqueeze(0)
        image = image.to(self.device)
        
        print("ðŸ” è¿è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹...")
        with torch.no_grad():
            output = self.model({'img': image})
            logits = output['logits']
            print(f"\nåŽŸå§‹logits: {logits}")
            probs = torch.softmax(logits, dim=1)
            real_prob = float(probs[0][0].cpu())
            fake_prob = float(probs[0][1].cpu())
            print(f"ðŸ“Š æ£€æµ‹ç»“æžœ - çœŸå®žæ¦‚çŽ‡: {real_prob:.4f}, ä¼ªé€ æ¦‚çŽ‡: {fake_prob:.4f}")
            is_fake = fake_prob > 0.5
            
        return is_fake, fake_prob, input_data_with_effect
    

    def extract_frames(self, video_path: str, num_frames: int = 30) -> List[Image.Image]:
        """ä»Žè§†é¢‘ä¸­å‡åŒ€æå–æŒ‡å®šæ•°é‡çš„å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
        
        frames = []
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = Image.fromarray(frame)
                frames.append(frame)
        
        cap.release()
        return frames
    

    # def predict_video(self, video_path: str, num_frames: int = 30) -> Tuple[bool, float, List[Tuple[bool, float]]]:
    #     """
    #     é¢„æµ‹è§†é¢‘æ˜¯å¦ä¸ºæ·±åº¦ä¼ªé€ 
    #     è¿”å›ž: (æ˜¯å¦ä¼ªé€ , ä¼ªé€ æ¦‚çŽ‡, æ¯å¸§çš„é¢„æµ‹ç»“æžœ)
    #     """
    #     frames = self.extract_frames(video_path, num_frames)
    #     frame_predictions = []
    #     fake_probs = []

    #     for frame in frames:
    #         is_fake, fake_prob = self.predict(frame)
    #         frame_predictions.append((is_fake, fake_prob))
    #         fake_probs.append(fake_prob)

    #     # è®¡ç®—å¹³å‡ä¼ªé€ æ¦‚çŽ‡
    #     avg_fake_prob = np.mean(fake_probs)
    #     is_fake = avg_fake_prob > 0.5

    #     return is_fake, avg_fake_prob, frame_predictions